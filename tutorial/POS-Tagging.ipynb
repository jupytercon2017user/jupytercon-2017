{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging: Agenda\n",
    "* <a href=\"#section1\">What are parts of speech? Why are they useful?</a>\n",
    "* <a href=\"#section2\">How do you use them with SpaCy?</a>\n",
    "* <a href=\"#section3\">How do we infer them?</a>\n",
    "* <a href=\"#section4\"> How do we learn them with SpaCy?</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_scratch():\n",
    "    !pip install spacy >> ~/spacy.log\n",
    "    !python -m spacy download en >> ~/spacy.log\n",
    "    !jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "\n",
    "    \n",
    "from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def as_list(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return list(f(*args, **kwargs))\n",
    "    \n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_(lambda x: x + 1, range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "from spacy.en import English\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from spacy.tokens import Doc\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "\n",
    "def as_list(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return list(f(*args, **kwargs))\n",
    "    \n",
    "    return wrapper\n",
    "    \n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "map_ = as_list(map)\n",
    "filter_ = as_list(filter)\n",
    "zip_ = as_list(zip)\n",
    "\n",
    "def rep_sentences(texts):\n",
    "    html = []\n",
    "    for text in texts:\n",
    "        html.append(rep_sentence(text))\n",
    "    return HTML(\"\".join(html))\n",
    "\n",
    "def rep_sentence(text, display_pos = True):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'purple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    doc = nlp(text)\n",
    "    n_words = len(doc)\n",
    "    unique_pos = list(set(map(lambda x: x.pos_, doc)))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    html.append(\"<tr>\")            \n",
    "    for i in range(n_words):\n",
    "        word_string= doc[i].orth_\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "    html.append(\"</tr>\")\n",
    "    if display_pos:\n",
    "        html.append(\"<tr>\")            \n",
    "        for i in range(n_words):\n",
    "            pos = doc[i].pos_\n",
    "            color = pos_to_color[pos]\n",
    "            html.append(\"<td><span class='{0}'>{0}</span></td>\".format(pos))\n",
    "        html.append(\"</tr>\")\n",
    "    html = \"\".join(html)\n",
    "    return html\n",
    "\n",
    "\n",
    "\n",
    "def custom_tag_table(list_of_word_tag_tuples):\n",
    "    html_colors = ['SkyBlue'\n",
    "               ,'red'\n",
    "               ,'YellowGreen'\n",
    "               ,'yellow'\n",
    "               ,'orange'\n",
    "               ,'pink'\n",
    "               ,'brown'\n",
    "               ,'MediumPurple'\n",
    "               , 'CadetBlue'\n",
    "                ,'DarkKhaki'\n",
    "                ,'DarkSalmon'\n",
    "                ,'Gold'    \n",
    "              ]\n",
    "    \n",
    "    n_words = len(list_of_word_tag_tuples)\n",
    "    words, pos_list = zip(*list_of_word_tag_tuples)\n",
    "    unique_pos = list(set([pos for pair in pos_list for pos in pair]))\n",
    "    pos_to_color = {i:html_colors[unique_pos.index(i)] for i in unique_pos}\n",
    "    css = [\"<style>.word{font-weight:bold;}</style>\"]\n",
    "    for pos in unique_pos:\n",
    "        css.append('<style>.{}{{background-color:{};}}</style>'.format(*[pos, pos_to_color[pos]]))\n",
    "    css = \"\".join(css)\n",
    "\n",
    "    html = [\"<table width=100%>\"]\n",
    "    html.append(css)\n",
    "    for i in range(n_words):\n",
    "        html.append(\"<tr>\")            \n",
    "        word_string= words[i]\n",
    "        html.append(\"<td><span class='word'>{0}</span></td>\".format(word_string))\n",
    "        row = []\n",
    "        pos_sublist = pos_list[i]\n",
    "        for pos in pos_sublist:\n",
    "            entry = \"<span class='{0}'>{0}</span> \".format(pos)\n",
    "            #print entry\n",
    "            row.append(entry)\n",
    "        row = \"\".join(row)\n",
    "        html.append(\"<td>{}</td>\".format(row))\n",
    "        html.append(\"</tr>\")\n",
    "    return \"\".join(html)\n",
    "        \n",
    "    \n",
    "\n",
    "def nltk_corpus(corpus_name):\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "#read nltk corpora\n",
    "def nltk_reader(corpus_name, limit = None):\n",
    "    corpus = nltk_corpus(corpus_name)\n",
    "    fileids = corpus.fileids()\n",
    "    \n",
    "    if limit:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids[:limit])\n",
    "    else:\n",
    "        doc_iter = (\" \".join([\" \".join(j) for j in corpus.sents(fileid)]) for fileid in fileids)\n",
    "    return doc_iter\n",
    "\n",
    "universal_tags = [\n",
    "     ['Open Class Words','ADJ','adjective']\n",
    "    ,['Open Class Words','ADV','adverb']\n",
    "    ,['Open Class Words','INTJ','interjection']\n",
    "    ,['Open Class Words','NOUN','noun']\n",
    "    ,['Open Class Words','PROPN','proper noun']\n",
    "    ,['Open Class Words','VERB','verb']\n",
    "    ,['Closed Class Words','ADP','adposition']\n",
    "    ,['Closed Class Words','AUX','auxiliary']\n",
    "    ,['Closed Class Words','CCONJ','coordination conjunction']\n",
    "    ,['Closed Class Words','DET','determiner']\n",
    "    ,['Closed Class Words','NUM','numeral']\n",
    "    ,['Closed Class Words','PART','particle']\n",
    "    ,['Closed Class Words','PRON','pronoun']\n",
    "    ,['Closed Class Words','SCONJ','subordinating conjection']\n",
    "    ,['Other','PUNCT','punctuation']\n",
    "    ,['Other','SYM','symbol']\n",
    "    ,['Other','X','other']\n",
    "]\n",
    "tag_table = pd.DataFrame(universal_tags, columns = ['Category','Abbrev','Part of Speech'])\n",
    "tag_table = tag_table.set_index(['Category','Abbrev'])\n",
    "\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section1\"></a>\n",
    "\n",
    "### What are Parts of Speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Part of Speech</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Abbrev</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">Open Class Words</th>\n",
       "      <th>ADJ</th>\n",
       "      <td>adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>interjection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>proper noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">Closed Class Words</th>\n",
       "      <th>ADP</th>\n",
       "      <td>adposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>auxiliary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>coordination conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>numeral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>particle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>subordinating conjection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Other</th>\n",
       "      <th>PUNCT</th>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>symbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Part of Speech\n",
       "Category           Abbrev                          \n",
       "Open Class Words   ADJ                    adjective\n",
       "                   ADV                       adverb\n",
       "                   INTJ                interjection\n",
       "                   NOUN                        noun\n",
       "                   PROPN                proper noun\n",
       "                   VERB                        verb\n",
       "Closed Class Words ADP                   adposition\n",
       "                   AUX                    auxiliary\n",
       "                   CCONJ   coordination conjunction\n",
       "                   DET                   determiner\n",
       "                   NUM                      numeral\n",
       "                   PART                    particle\n",
       "                   PRON                     pronoun\n",
       "                   SCONJ   subordinating conjection\n",
       "Other              PUNCT                punctuation\n",
       "                   SYM                       symbol\n",
       "                   X                          other"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.VERB{background-color:SkyBlue;}</style><style>.DET{background-color:red;}</style><style>.NOUN{background-color:YellowGreen;}</style><style>.PRON{background-color:yellow;}</style><style>.PUNCT{background-color:orange;}</style><style>.ADP{background-color:pink;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>get</span></td><td><span class='word'>a</span></td><td><span class='word'>discount</span></td><td><span class='word'>on</span></td><td><span class='word'>newspapers</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='DET'>DET</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr><table width=100%><style>.word{font-weight:bold;}</style><style>.PUNCT{background-color:SkyBlue;}</style><style>.PRON{background-color:red;}</style><style>.VERB{background-color:YellowGreen;}</style><style>.ADP{background-color:yellow;}</style><style>.NOUN{background-color:orange;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>discount</span></td><td><span class='word'>that</span></td><td><span class='word'>newspaper</span></td><td><span class='word'>.</span></td></tr><tr><td><span class='PRON'>PRON</span></td><td><span class='VERB'>VERB</span></td><td><span class='ADP'>ADP</span></td><td><span class='NOUN'>NOUN</span></td><td><span class='PUNCT'>PUNCT</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = 'I get a discount on newspapers.'\n",
    "sentence2 = 'I discount that newspaper.'\n",
    "\n",
    "rep_sentences([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='applications'></a>\n",
    "### Applications\n",
    "* Rule based systems:\n",
    "    * <a href=\"#qacode\">Example of rule based question answering component</a>\n",
    "* Feature engineering for statistical models\n",
    "    * <a href=\"#wordsense\">Feature for word disambiguation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section2\"></a>\n",
    "### Parts of Speech with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos (coarse)</th>\n",
       "      <th>pos (fine)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discount</th>\n",
       "      <td>discount</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>get</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newspapers</th>\n",
       "      <td>newspaper</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                lemma pos (coarse) pos (fine)\n",
       "I              -PRON-         PRON        PRP\n",
       "a                   a          DET         DT\n",
       "discount     discount         NOUN         NN\n",
       "get               get         VERB        VBP\n",
       "newspapers  newspaper         NOUN        NNS\n",
       "on                 on          ADP         IN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Accessing\n",
    "doc = nlp('I get a discount on newspapers')\n",
    "tags = {}\n",
    "\n",
    "for word in doc:\n",
    "    tags[word.orth_] = {'lemma': word.lemma_, \n",
    "                        'pos (coarse)': word.pos_, \n",
    "                        'pos (fine)':word.tag_}\n",
    "pd.DataFrame(tags).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Building word vectors that are Part of Speech specific\n",
    "Steps:\n",
    "* get documents\n",
    "* tokenize the documents, and append the part of speech to each token, e.g. dog|NOUN\n",
    "* train a word2vec model with gensim\n",
    "* compare the most similar words of 'back||||VERB' vs 'back||||NOUN' (or other combo)\n",
    "\n",
    "* Hints:\n",
    "    * model.wv.vocab contains the vocabulary.\n",
    "    * using a completely unique join character will make it easier to split later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def return_documents():\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    dataset = fetch_20newsgroups()\n",
    "    corpus = dataset.data\n",
    "    return corpus\n",
    "\n",
    "def tokenize_and_tag_documents(documents, nlp, sep_char=\"|\"):\n",
    "    pass\n",
    "\n",
    "def build_model(tokenized_docs):\n",
    "    pass\n",
    "\n",
    "def compare_most_similar_words_across_pos(word):\n",
    "    pass\n",
    "\n",
    "documents = return_documents()\n",
    "tokenized_and_tagged_documents = tokenize_and_tag_documents(documents, nlp)\n",
    "model = build_model(tokenized_and_tagged_documents)\n",
    "compare_most_similar_words_across_pos('back')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section3\"></a>\n",
    "### How do we infer parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><style>.word{font-weight:bold;}</style><style>.VERB{background-color:SkyBlue;}</style><style>.PROPN{background-color:red;}</style><style>.DET{background-color:YellowGreen;}</style><style>.ADJ{background-color:yellow;}</style><style>.NOUN{background-color:orange;}</style><style>.PRON{background-color:pink;}</style><style>.PUNCT{background-color:brown;}</style><style>.ADP{background-color:purple;}</style><style>.PART{background-color:CadetBlue;}</style><tr><td><span class='word'>I</span></td><td><span class='word'>was</span></td><td><span class='word'>loble</span></td><td><span class='word'>to</span></td><td><span class='word'>find</span></td><td><span class='word'>the</span></td><td><span class='word'>effix</span></td><td><span class='word'>by</span></td><td><span class='word'>klepping</span></td><td><span class='word'>the</span></td><td><span class='word'>Dongle</span></td><td><span class='word'>search</span></td><td><span class='word'>engine</span></td><td><span class='word'>.</span></td></tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import Button\n",
    "class reveal(object):\n",
    "    def __init__(self):\n",
    "        self.text = 'I was loble to find the effix by klepping the Dongle search engine.'\n",
    "        self.toggle = Button(description='Toggle POS', )\n",
    "        self.toggle.on_click(self.toggle_pos)\n",
    "        self.state = False\n",
    "        display(self.toggle)\n",
    "        self.display()\n",
    "        \n",
    "    def toggle_pos(self, b):\n",
    "        self.state = not self.state\n",
    "        self.display()\n",
    "        \n",
    "    def display(self):\n",
    "        clear_output()\n",
    "        display(HTML(rep_sentence(self.text, display_pos = self.state)))\n",
    "        \n",
    "r = reveal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinants of Part of Speech:\n",
    "* Word: some words can only be used in a single way; we can memorize these.\n",
    "* Word shape: if the first letter is capitalized, its likely a proper noun.\n",
    "* Neighboring part of speech: there are common patterns, such as noun phrases commonly following a determiner. to the beach\n",
    "\n",
    "\n",
    "\n",
    "| Feature | Notes | Example|\n",
    "|------|------|------|\n",
    "|   Word Identity  | Some words can only be used in a single way; we can memorize these.| \"the\" -> determiner| \n",
    "| Word Shape|Capitalization, dashes,  |\"I stayed at the Park Hotel.\"|\n",
    "|Neighboring parts of speech|There are common patterns what tags can neighbor others|\"to the beach\" (noun following determiner)|\n",
    "|Morphological Structures|Word prefixes and suffixes can rule out certain tag types|\"-ly\" -> adverb|\n",
    "|Syntactic Dependencies|Syntax may establish expectations that only certain tags can logically fill|\"I was told __\" -> adpositional phrase or object entity|\n",
    "|?|?|?|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section4\"></a>\n",
    "### Training your own tagger\n",
    "\n",
    "**Steps** : \n",
    "* load <a href=\"#load_data\">**training data**</a>, where each observation is represented as (list_of_words, list_of_tags)\n",
    "* Pick a <a href=\"#model_dir\">**model directory**</a>. Using the existing English model will allow us to leverage lexeme information, including Brown clusters, which is an excellent feature for tagging.\n",
    "* Look at your dataset. Build a <a href=\"#tagmap\">**tag map**</a> mapping from the part of speech tags to the <a href=\"#universaltagset\">universal tagset</a>.\n",
    "* Decide which <a href=\"#featureextractors\">**features**</a> to use\n",
    "* create a <a href=\"#vocab\">**vocabulary object, a statistical model, and a tagger**</a>\n",
    "* <a href=\"#training\">**Train the model**</a>\n",
    "* <a href=\"#save\">**Save the model**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"load_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load conll2000 corpus\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "def nltk_corpus(corpus_name):\n",
    "    '''returns nltk corpus by name. if not loaded, download.'''\n",
    "    corpus = getattr(nltk.corpus, corpus_name)\n",
    "    try:\n",
    "        corpus.ensure_loaded()\n",
    "    except:\n",
    "        nltk.download(corpus_name)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    if x == '-LCB-':\n",
    "        return '{'\n",
    "    elif x=='-RCB-':\n",
    "        return '}'\n",
    "    elif x == '-RRB-':\n",
    "        return \")\"\n",
    "    elif x == '-LRB-':\n",
    "        return \"(\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def conll_to_data():\n",
    "    corpus = nltk_corpus('conll2000')\n",
    "    all_data= map_(lambda x: [(clean(i[0]), i[1]) for i in x], corpus.iob_sents())\n",
    "    all_data = [zip_(*i) for i in all_data]\n",
    "    return all_data\n",
    "\n",
    "c2000 = conll_to_data()\n",
    "training_data, testing_data = train_test_split(c2000, test_size = .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model_dir\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "modelpath = PosixPath('/home/jupyter/mymodel')\n",
    "if not modelpath.exists():\n",
    "    modelpath.mkdir()\n",
    "    \n",
    "nlp.save_to_directory(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"tagmap\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.language_data import TAG_MAP\n",
    "from spacy.attrs import POS\n",
    "from spacy.symbols import PUNCT\n",
    "\n",
    "def adjust_tagmap(tagmap):\n",
    "    tagmap['('] = tagmap['-LRB-']\n",
    "    tagmap[')'] = tagmap['-RRB-']\n",
    "    tagmap['{'] = tagmap['-LRB-']\n",
    "    tagmap['}'] = tagmap['-RRB-']\n",
    "    tagmap['$'] = {POS: PUNCT}\n",
    "    return tagmap\n",
    "\n",
    "tagmap = adjust_tagmap(TAG_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"featureextractors\"></a>\n",
    "### Features\n",
    "\n",
    "* Example features: pos of previous word, identity of current word, etc...\n",
    "* spacy.tagger.N1_cluster, spacy.tagger.N1_pos, etc...\n",
    "* which word?\n",
    "    * N1: Next\n",
    "    * N0: Current\n",
    "    * P1: Previous\n",
    "    * etc...\n",
    "* which attribute:\n",
    "    * prefix\n",
    "    * tag\n",
    "    * cluster\n",
    "    * etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tagger import *\n",
    "\n",
    "\n",
    "features = [\n",
    "    #current word attributes\n",
    "    (W_orth,),(W_shape,),(W_cluster,),(W_flags,),(W_suffix,),(W_prefix,),\n",
    "\n",
    "    #-1 word attributes    \n",
    "    (P1_pos,),(P1_cluster,),(P1_flags,),(P1_suffix,),\n",
    "\n",
    "    #-2 word attributes     \n",
    "    (P2_pos,),(P2_cluster,),(P2_flags,),\n",
    "\n",
    "    #+1 word attributes    \n",
    "    (N1_orth,),(N1_suffix,),(N1_cluster,),(N1_flags,),    \n",
    "\n",
    "    #+2 word attributes    \n",
    "    (N2_orth,),(N2_cluster,),(N2_flags,),\n",
    "\n",
    "    #combination attributes\n",
    "    (P1_lemma, P1_pos),(P2_lemma, P2_pos), (P1_pos, P2_pos),(P1_pos, W_orth)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"vocab\"></a>\n",
    "### Vocabulary, Tagger, and Statistical Model\n",
    "* The **Vocab** object will receive all the lexeme data (Brown clusters, word vectors, etc) from the English model.\n",
    "* The **Statistical Model**  will consume the features we defined, using them to make predictions.\n",
    "* The **Tagger** will consume our vocabulary object, and our statistical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_lemmatizer():\n",
    "    return None\n",
    "\n",
    "def make_tagger(vocab, templates):\n",
    "    model = spacy.tagger.TaggerModel(templates)\n",
    "    return spacy.tagger.Tagger(vocab,model)\n",
    "\n",
    "def get_feature_extractors(nlp):\n",
    "    #return nlp.vocab.lex_attr_getters\n",
    "    return nlp.Defaults.lex_attr_getters\n",
    "\n",
    "#get requirements\n",
    "\n",
    "feature_extractors = get_feature_extractors(nlp)\n",
    "lemmatizer = get_lemmatizer()\n",
    "vocab = Vocab.load(modelpath, feature_extractors, lemmatizer, tagmap)\n",
    "statistical_model = spacy.tagger.TaggerModel(features)\n",
    "tagger = spacy.tagger.Tagger(vocab, statistical_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PUNCT'),\n",
       " ('can', 'PUNCT'),\n",
       " ('always', 'PUNCT'),\n",
       " ('learn', 'PUNCT'),\n",
       " ('more', 'PUNCT'),\n",
       " ('by', 'PUNCT'),\n",
       " ('reading', 'PUNCT')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The untrained tagger is awful...\n",
    "words = ['You','can','always','learn','more','by','reading']\n",
    "doc = Doc(vocab, words = words)\n",
    "tagger(doc)\n",
    "map_(lambda x: (x.orth_, x.pos_), doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"training\"></a>\n",
    "### Training the Model\n",
    "\n",
    "##### Neuron Prediction:\n",
    "\n",
    "**Inputs:** \n",
    "\n",
    "$<x_1, x_2, ..., x_n>$\n",
    "\n",
    "**Each Neuron j:**\n",
    "\n",
    "$prediction_j  = \\bigg[\\sum_{d=1}^D w_dx_{dj} \\bigg] + b > 0$\n",
    "\n",
    "##### Perceptron Learning:\n",
    "```\n",
    "For each document, label:\n",
    "    prediction = weights * document + bias\n",
    "    \n",
    "    if sign(label) != sign(prediction):\n",
    "        weights = weights + (label*features)\n",
    "        bias = bias + label\n",
    "```\n",
    "\n",
    "### Exercise: \n",
    "Implement a perceptron algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "class PerceptronClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.historical_weights = {}\n",
    "        self.iter = 0\n",
    "\n",
    "    def fit(self, X, y, epochs=100):\n",
    "        \"\"\"Fits self.weights, self.biases \"\"\"\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            for x, label in zip(X, y):\n",
    "                \n",
    "                prediction = self.predict(x)\n",
    "                if np.sign(prediction) != np.sign(label):\n",
    "                    self.update(label, x)\n",
    "                    self.historical_weights[self.iter] = self.weights\n",
    "                self.iter += 1\n",
    "                \n",
    "        \n",
    "            X, y = shuffle(X, y)\n",
    "                    \n",
    "    def update(self, label, row):\n",
    "        \"\"\"Updates weights and biases based on the ground truth label\n",
    "        and the row\"\"\"\n",
    "        self.weights = self.weights + (label * row)\n",
    "        self.bias += self.bias + label\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Generates 1s and 0s by doing a linear transformation \n",
    "        of \"x\". Uses self.weights and self.bias\"\"\"\n",
    "        pred = ((np.dot(x, self.weights)) + self.bias > 0).astype(float)\n",
    "        return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Rate: 0.391\n"
     ]
    }
   ],
   "source": [
    "#generate some data\n",
    "X = np.random.normal(0, 10, size=(1000, 3))\n",
    "true_b = np.array([1.2, -1.2, 0])\n",
    "labels = (np.dot(X, true_b) > 5.0).astype(float)\n",
    "\n",
    "b = PerceptronClassifier()\n",
    "b.fit(X, labels, epochs=1000)\n",
    "\n",
    "acc_rate = (b.predict(X) == labels).mean()\n",
    "print(\"Training Accuracy Rate: {}\".format(acc_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999970</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999971</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999972</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999973</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999974</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999975</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999976</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999977</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999978</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999979</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999980</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999981</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999982</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999983</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999984</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999985</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999986</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999987</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999988</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999989</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999990</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999991</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999992</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999993</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999994</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>32.461313</td>\n",
       "      <td>-32.860281</td>\n",
       "      <td>6.467768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0          1         2\n",
       "0       32.461313 -32.860281  6.467768\n",
       "1       32.461313 -32.860281  6.467768\n",
       "2       32.461313 -32.860281  6.467768\n",
       "3       32.461313 -32.860281  6.467768\n",
       "4       32.461313 -32.860281  6.467768\n",
       "5       32.461313 -32.860281  6.467768\n",
       "6       32.461313 -32.860281  6.467768\n",
       "7       32.461313 -32.860281  6.467768\n",
       "8       32.461313 -32.860281  6.467768\n",
       "9       32.461313 -32.860281  6.467768\n",
       "10      32.461313 -32.860281  6.467768\n",
       "11      32.461313 -32.860281  6.467768\n",
       "12      32.461313 -32.860281  6.467768\n",
       "13      32.461313 -32.860281  6.467768\n",
       "14      32.461313 -32.860281  6.467768\n",
       "15      32.461313 -32.860281  6.467768\n",
       "16      32.461313 -32.860281  6.467768\n",
       "17      32.461313 -32.860281  6.467768\n",
       "18      32.461313 -32.860281  6.467768\n",
       "19      32.461313 -32.860281  6.467768\n",
       "20      32.461313 -32.860281  6.467768\n",
       "21      32.461313 -32.860281  6.467768\n",
       "22      32.461313 -32.860281  6.467768\n",
       "23      32.461313 -32.860281  6.467768\n",
       "24      32.461313 -32.860281  6.467768\n",
       "25      32.461313 -32.860281  6.467768\n",
       "26      32.461313 -32.860281  6.467768\n",
       "27      32.461313 -32.860281  6.467768\n",
       "28      32.461313 -32.860281  6.467768\n",
       "29      32.461313 -32.860281  6.467768\n",
       "...           ...        ...       ...\n",
       "999970  32.461313 -32.860281  6.467768\n",
       "999971  32.461313 -32.860281  6.467768\n",
       "999972  32.461313 -32.860281  6.467768\n",
       "999973  32.461313 -32.860281  6.467768\n",
       "999974  32.461313 -32.860281  6.467768\n",
       "999975  32.461313 -32.860281  6.467768\n",
       "999976  32.461313 -32.860281  6.467768\n",
       "999977  32.461313 -32.860281  6.467768\n",
       "999978  32.461313 -32.860281  6.467768\n",
       "999979  32.461313 -32.860281  6.467768\n",
       "999980  32.461313 -32.860281  6.467768\n",
       "999981  32.461313 -32.860281  6.467768\n",
       "999982  32.461313 -32.860281  6.467768\n",
       "999983  32.461313 -32.860281  6.467768\n",
       "999984  32.461313 -32.860281  6.467768\n",
       "999985  32.461313 -32.860281  6.467768\n",
       "999986  32.461313 -32.860281  6.467768\n",
       "999987  32.461313 -32.860281  6.467768\n",
       "999988  32.461313 -32.860281  6.467768\n",
       "999989  32.461313 -32.860281  6.467768\n",
       "999990  32.461313 -32.860281  6.467768\n",
       "999991  32.461313 -32.860281  6.467768\n",
       "999992  32.461313 -32.860281  6.467768\n",
       "999993  32.461313 -32.860281  6.467768\n",
       "999994  32.461313 -32.860281  6.467768\n",
       "999995  32.461313 -32.860281  6.467768\n",
       "999996  32.461313 -32.860281  6.467768\n",
       "999997  32.461313 -32.860281  6.467768\n",
       "999998  32.461313 -32.860281  6.467768\n",
       "999999  32.461313 -32.860281  6.467768\n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array(b.historical_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\nB//Z\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/JD_NtVl7o8c\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7ff83eb58780>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('JD_NtVl7o8c', width=560, height=315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Training Accuracy: 0.0 Pre Test Accuracy 0.0\n",
      "Iteration 0 Training Accuracy: 94.6284744833259 Test Accuracy 93.4171597633136\n",
      "Iteration 1 Training Accuracy: 96.83783459574322 Test Accuracy 95.48816568047337\n",
      "Iteration 2 Training Accuracy: 98.09396099667546 Test Accuracy 96.5587044534413\n",
      "Iteration 3 Training Accuracy: 98.58107413373548 Test Accuracy 96.7650264715042\n",
      "Iteration 4 Training Accuracy: 99.12859444082667 Test Accuracy 97.38788539395827\n",
      "Iteration 5 Training Accuracy: 99.1024608424444 Test Accuracy 97.12706322018063\n",
      "Iteration 6 Training Accuracy: 99.62856016725503 Test Accuracy 97.71099345998131\n",
      "Iteration 7 Training Accuracy: 99.6088528635569 Test Accuracy 97.59810028028652\n",
      "Iteration 8 Training Accuracy: 99.43791342495801 Test Accuracy 97.38009965742759\n",
      "Iteration 9 Training Accuracy: 99.70096308736333 Test Accuracy 97.67595764559327\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "\n",
    "def predict_cycle(vocab, tagger, data, train=True):\n",
    "    \"\"\"For each document in data, creates a document and tags\n",
    "    it. Creates a goldparse object to hold the ground truth label.\n",
    "    If train=True, the tagger's statistical model is updated with the \n",
    "    result.\"\"\"\n",
    "    \n",
    "    scorer = Scorer()\n",
    "    \n",
    "    for words, tags in data:\n",
    "        #create a document, passing in words to become tokens\n",
    "        doc = Doc(vocab, words = words)\n",
    "        tagger(doc)\n",
    "        gold = GoldParse(doc, tags=tags)   \n",
    "\n",
    "        scorer.score(doc, gold)              \n",
    "        \n",
    "        if train:\n",
    "            #train the model        \n",
    "            tagger.update(doc, gold)\n",
    "    return tagger, scorer\n",
    "\n",
    "def train(vocab, tagger, training_data, testing_data, epochs = 10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tagger (spacy.tagger.Tagger): \n",
    "        The tagger to train.\n",
    "        \n",
    "    training_data (list):\n",
    "        Training data containing words and annotated tags. \n",
    "        Should have form: [(word1, word2,...),(tag1, tag2, .....)]\n",
    "        \n",
    "    epochs (int):\n",
    "        number of training iterations\n",
    "        \n",
    "    verbose (Bool):\n",
    "        whether to track and print training accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    tagger, pre_train_scorer = predict_cycle(vocab, tagger, training_data, train=False)\n",
    "    tagger, pre_test_scorer = predict_cycle(vocab, tagger, testing_data, train=False)\n",
    "    \n",
    "    print(\"Pre Training Accuracy: {0} Pre Test Accuracy {1}\".format(pre_train_scorer.tags_acc, \n",
    "                                                                    pre_test_scorer.tags_acc))\n",
    "    \n",
    "    for train_cycle in range(epochs):\n",
    "            \n",
    "        tagger, _ = predict_cycle(vocab, tagger, training_data, train=True)\n",
    "        \n",
    "        \n",
    "        tagger, train_scorer = predict_cycle(vocab, tagger, training_data, train=False)\n",
    "        tagger, test_scorer = predict_cycle(vocab, tagger, testing_data, train=False)\n",
    "\n",
    "        print(\"Iteration {0} Training Accuracy: {1} Test Accuracy {2}\".format(train_cycle, \n",
    "                                                                              train_scorer.tags_acc, \n",
    "                                                                              test_scorer.tags_acc))\n",
    "        #shuffle data    \n",
    "        np.random.shuffle(training_data)\n",
    "    \n",
    "    tagger.model.end_training()\n",
    "        \n",
    "    return tagger\n",
    "\n",
    "tagger = train(vocab, tagger, training_data, testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"save\"></a>\n",
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tagger(tagger, model_dir):\n",
    "    if model_dir is not None:\n",
    "        tagger.model.dump(str(model_dir / 'pos' / 'model'))\n",
    "        with (model_dir / 'vocab' / 'strings.json').open('w') as file_:\n",
    "            tagger.vocab.strings.dump(file_)\n",
    "            \n",
    "save_tagger(tagger, modelpath)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='qacode'></a>\n",
    "### Example Rule Based QA Component Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_answer_requirements(token):\n",
    "    if token.tag_ == 'WRB':\n",
    "        if token.lower_ == 'where':\n",
    "            #Where was Star Wars Filmed\n",
    "            return ['LOCATION']\n",
    "        elif token.lower_ == 'when':\n",
    "            #When was Star Wars Filmed\n",
    "            return ['DATE']\n",
    "        elif token.lower_ == 'how':\n",
    "            #How much did Star Wars make?\n",
    "            if token.nbor().lower_ in ('much', 'many'):\n",
    "                return ['QUANTITY']\n",
    "\n",
    "            #How old is star wars?\n",
    "            elif token.nbor().lower_ in ('long', 'old'):\n",
    "                return ['DURATION']\n",
    "            else:\n",
    "                return False\n",
    "        elif token.lower() == 'whom':\n",
    "            #Whom did you see?\n",
    "            return ['PERSON','ORG']      \n",
    "        else:\n",
    "            return False\n",
    "    elif token.tag_ == 'WP':\n",
    "        #Asking for Identity\n",
    "        if token.lower_ in ('who', 'whose'):\n",
    "            #Who directed Star Wars?\n",
    "            return ['PERSON','ORG']\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #What is Star Wars\n",
    "            return False \n",
    "        else: \n",
    "            return False\n",
    "    elif token.tag_ == 'WDT':\n",
    "        #asking for a choice among options\n",
    "        if token.lower_ in ('which','what'):\n",
    "            #which Star Wars did you like best?\n",
    "            return [token.nbor().lower_] #return neighbor\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>\n",
    "<a name=\"wordsense\"></a>\n",
    "##### Word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Synset('shower.n.01') a plumbing fixture that sprays water over you\n",
      "Synset('shower.n.02') washing yourself by standing upright under water sprayed from a nozzle\n",
      "Synset('shower.n.03') a brief period of precipitation\n",
      "Synset('shower.n.04') a sudden downpour (as of tears or sparks etc) likened to a rain shower\n",
      "Synset('exhibitor.n.01') someone who organizes an exhibit for others to see\n",
      "Synset('shower.n.06') a party of friends assembled to present gifts (usually of a specified kind) to a person\n",
      "Synset('lavish.v.01') expend profusely; also used with abstract nouns\n",
      "Synset('shower.v.02') spray or sprinkle with\n",
      "Synset('shower.v.03') take a shower; wash one's body in the shower\n",
      "Synset('shower.v.04') rain abundantly\n",
      "Synset('shower.v.05') provide abundantly with\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "for syn in wn.synsets('shower'):\n",
    "    print(syn, syn.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#applications'>back</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
